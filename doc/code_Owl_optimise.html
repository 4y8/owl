<html><head>
<link rel="stylesheet" href="style.css" type="text/css">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<link rel="Start" href="index.html">
<link title="Index of types" rel=Appendix href="index_types.html">
<link title="Index of values" rel=Appendix href="index_values.html">
<link title="Index of modules" rel=Appendix href="index_modules.html">
<link title="Owl" rel="Chapter" href="Owl.html">
<link title="Owl_types" rel="Chapter" href="Owl_types.html">
<link title="Owl_utils" rel="Chapter" href="Owl_utils.html">
<link title="Owl_maths" rel="Chapter" href="Owl_maths.html">
<link title="Owl_stats" rel="Chapter" href="Owl_stats.html">
<link title="Owl_optimise" rel="Chapter" href="Owl_optimise.html">
<link title="Owl_dense_real" rel="Chapter" href="Owl_dense_real.html">
<link title="Owl_dense_complex" rel="Chapter" href="Owl_dense_complex.html">
<link title="Owl_dense" rel="Chapter" href="Owl_dense.html">
<link title="Owl_sparse_real" rel="Chapter" href="Owl_sparse_real.html">
<link title="Owl_sparse_complex" rel="Chapter" href="Owl_sparse_complex.html">
<link title="Owl_sparse" rel="Chapter" href="Owl_sparse.html">
<link title="Owl_foreign" rel="Chapter" href="Owl_foreign.html">
<link title="Owl_linalg" rel="Chapter" href="Owl_linalg.html">
<link title="Owl_regression" rel="Chapter" href="Owl_regression.html">
<link title="Owl_pretty" rel="Chapter" href="Owl_pretty.html">
<link title="Owl_plot" rel="Chapter" href="Owl_plot.html">
<link title="Owl_toplevel" rel="Chapter" href="Owl_toplevel.html">
<link title="Owl_const" rel="Chapter" href="Owl_const.html">
<link title="Ffi_bindings" rel="Chapter" href="Ffi_bindings.html"><title>Owl_optimise</title>
</head>
<body>
<code class="code"><span class="comment">(*
&nbsp;*&nbsp;OWL&nbsp;-&nbsp;an&nbsp;OCaml&nbsp;math&nbsp;library&nbsp;for&nbsp;scientific&nbsp;computing
&nbsp;*&nbsp;Copyright&nbsp;(c)&nbsp;2016&nbsp;Liang&nbsp;Wang&nbsp;&lt;liang.wang@cl.cam.ac.uk&gt;
&nbsp;*)</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code">
  <span class="constructor">Machine</span> learning library
  <span class="constructor">Note</span>: <span class="constructor">C</span> layout row-major matrix
  </code>  *)</span></td></tr></table><code class="code">

<span class="keyword">module</span>&nbsp;<span class="constructor">MX</span>&nbsp;=&nbsp;<span class="constructor">Owl_dense_real</span>
<span class="keyword">module</span>&nbsp;<span class="constructor">UT</span>&nbsp;=&nbsp;<span class="constructor">Owl_utils</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code">
  <span class="constructor">K</span>-means clustering algorithm
  x is the row-based data points <span class="keyword">and</span> c is the number <span class="keyword">of</span> clusters.
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;kmeans&nbsp;x&nbsp;c&nbsp;=&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;cpts0&nbsp;=&nbsp;fst&nbsp;(draw_rows&nbsp;x&nbsp;c)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;cpts1&nbsp;=&nbsp;zeros&nbsp;c&nbsp;(col_num&nbsp;x)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;assignment&nbsp;=&nbsp;<span class="constructor">Array</span>.make&nbsp;(row_num&nbsp;x)&nbsp;(0,&nbsp;max_float)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;<span class="keyword">try</span>&nbsp;<span class="keyword">for</span>&nbsp;counter&nbsp;=&nbsp;1&nbsp;<span class="keyword">to</span>&nbsp;100&nbsp;<span class="keyword">do</span>
&nbsp;&nbsp;<span class="constructor">Log</span>.info&nbsp;<span class="string">"iteration&nbsp;%i&nbsp;..."</span>&nbsp;counter;&nbsp;flush&nbsp;stdout;
&nbsp;&nbsp;iteri_rows&nbsp;(<span class="keyword">fun</span>&nbsp;i&nbsp;v&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;iteri_rows&nbsp;(<span class="keyword">fun</span>&nbsp;j&nbsp;u&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;e&nbsp;=&nbsp;sum((v&nbsp;-@&nbsp;u)&nbsp;**@&nbsp;2.)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;e&nbsp;&lt;&nbsp;snd&nbsp;assignment.(i)&nbsp;<span class="keyword">then</span>&nbsp;assignment.(i)&nbsp;&lt;-&nbsp;(j,&nbsp;e)
&nbsp;&nbsp;&nbsp;&nbsp;)&nbsp;cpts0
&nbsp;&nbsp;)&nbsp;x;
&nbsp;&nbsp;iteri_rows&nbsp;(<span class="keyword">fun</span>&nbsp;j&nbsp;u&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;<span class="constructor">UT</span>.filteri_array&nbsp;(<span class="keyword">fun</span>&nbsp;i&nbsp;y&nbsp;<span class="keywordsign">-&gt;</span>&nbsp;fst&nbsp;y&nbsp;=&nbsp;j,&nbsp;i)&nbsp;assignment&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;average_rows&nbsp;(rows&nbsp;x&nbsp;l)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;copy_row_to&nbsp;z&nbsp;cpts1&nbsp;j&nbsp;<span class="keyword">in</span>&nbsp;()
&nbsp;&nbsp;)&nbsp;cpts0;
&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;cpts0&nbsp;=@&nbsp;cpts1&nbsp;<span class="keyword">then</span>&nbsp;failwith&nbsp;<span class="string">"converged"</span>&nbsp;<span class="keyword">else</span>&nbsp;ignore&nbsp;(cpts0&nbsp;&lt;&lt;&nbsp;cpts1)
&nbsp;&nbsp;<span class="keyword">done</span>&nbsp;<span class="keyword">with</span>&nbsp;exn&nbsp;<span class="keywordsign">-&gt;</span>&nbsp;()&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;cpts1,&nbsp;<span class="constructor">UT</span>.map_array&nbsp;fst&nbsp;assignment


</code><table><tr><td></td><td><span class="comment">(** <code class="code">
  a numberical way <span class="keyword">of</span> calculating gradient.
  x is a k x m matrix containing m classifiers <span class="keyword">of</span> k features.
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;numerical_gradient&nbsp;l&nbsp;p&nbsp;x&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;h&nbsp;=&nbsp;0.00001&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;f&nbsp;=&nbsp;l&nbsp;y&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;mapi_by_row&nbsp;~d:(col_num&nbsp;p)
&nbsp;&nbsp;(<span class="keyword">fun</span>&nbsp;i&nbsp;v&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;fa&nbsp;=&nbsp;f&nbsp;(x&nbsp;$@&nbsp;(replace_row&nbsp;(v&nbsp;-$&nbsp;h)&nbsp;p&nbsp;i))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;fb&nbsp;=&nbsp;f&nbsp;(x&nbsp;$@&nbsp;(replace_row&nbsp;(v&nbsp;+$&nbsp;h)&nbsp;p&nbsp;i))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;(fb&nbsp;-@&nbsp;fa)&nbsp;/$&nbsp;(2.&nbsp;*.&nbsp;h)
&nbsp;&nbsp;)&nbsp;p&nbsp;<span class="keyword">in</span>&nbsp;g


<span class="comment">(*&nbsp;Regularisation&nbsp;functions&nbsp;*)</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">L1</span> regularisation <span class="keyword">and</span> its subgradient </code> *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;l1&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(average_rows&nbsp;(abs&nbsp;p))

<span class="keyword">let</span>&nbsp;l1_grad&nbsp;p&nbsp;=&nbsp;&nbsp;<span class="comment">(*&nbsp;TODO:&nbsp;I&nbsp;may&nbsp;change&nbsp;it&nbsp;to&nbsp;noisy&nbsp;unbiased&nbsp;subgradient&nbsp;in&nbsp;future&nbsp;*)</span>
&nbsp;&nbsp;<span class="constructor">MX</span>.map&nbsp;(<span class="keyword">fun</span>&nbsp;x&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;x&nbsp;&gt;&nbsp;0.&nbsp;<span class="keyword">then</span>&nbsp;1.&nbsp;<span class="keyword">else</span>&nbsp;<span class="keyword">if</span>&nbsp;x&nbsp;&lt;&nbsp;0.&nbsp;<span class="keyword">then</span>&nbsp;(-1.)&nbsp;<span class="keyword">else</span>&nbsp;0.
&nbsp;&nbsp;)&nbsp;p

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">L2</span> regularisation <span class="keyword">and</span> its grandient </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;l2&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(0.5&nbsp;$*&nbsp;(average_rows&nbsp;(p&nbsp;*@&nbsp;p)))

<span class="keyword">let</span>&nbsp;l2_grad&nbsp;p&nbsp;=&nbsp;p

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Elastic</span> net regularisation <span class="keyword">and</span> its gradient
  <span class="string">"a"</span> is the weight on l1 regularisation term. </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;elastic&nbsp;a&nbsp;x&nbsp;=&nbsp;<span class="constructor">MX</span>.(a&nbsp;$*&nbsp;(l1&nbsp;x)&nbsp;+@&nbsp;((1.&nbsp;-.&nbsp;a)&nbsp;$*&nbsp;(l2&nbsp;x)))

<span class="keyword">let</span>&nbsp;elastic_grad&nbsp;a&nbsp;x&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g1&nbsp;=&nbsp;l1_grad&nbsp;x&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g2&nbsp;=&nbsp;l2_grad&nbsp;x&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="constructor">MX</span>.(a&nbsp;$*&nbsp;g1&nbsp;+@&nbsp;(a&nbsp;$*&nbsp;g2))

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">No</span> regularisation <span class="keyword">and</span> its gradient </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;noreg&nbsp;x&nbsp;=&nbsp;<span class="constructor">MX</span>.(zeros&nbsp;1&nbsp;(col_num&nbsp;x))

<span class="keyword">let</span>&nbsp;noreg_grad&nbsp;x&nbsp;=&nbsp;<span class="constructor">MX</span>.(zeros&nbsp;(row_num&nbsp;x)&nbsp;(col_num&nbsp;x))


<span class="comment">(*&nbsp;Loss&nbsp;functions&nbsp;*)</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> least square loss <span class="keyword">function</span> </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;square_loss&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;average_rows&nbsp;((y'&nbsp;-@&nbsp;y)&nbsp;**@&nbsp;2.)

<span class="keyword">let</span>&nbsp;square_grad&nbsp;x&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;(transpose&nbsp;x)&nbsp;$@&nbsp;(y'&nbsp;-@&nbsp;y)&nbsp;/$&nbsp;(float_of_int&nbsp;(row_num&nbsp;x))

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> hinge loss <span class="keyword">function</span> </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;hinge_loss&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;1.&nbsp;$-&nbsp;(&nbsp;y&nbsp;*@&nbsp;y'&nbsp;)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;map&nbsp;(<span class="constructor">Pervasives</span>.max&nbsp;0.)&nbsp;z&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;average_rows&nbsp;z

<span class="keyword">let</span>&nbsp;hinge_grad&nbsp;x&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;mapi&nbsp;(<span class="keyword">fun</span>&nbsp;i&nbsp;j&nbsp;x&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;x&nbsp;&lt;&nbsp;1.&nbsp;<span class="keyword">then</span>&nbsp;(0.&nbsp;-.&nbsp;y.{i,j})&nbsp;<span class="keyword">else</span>&nbsp;0.
&nbsp;&nbsp;)&nbsp;(y&nbsp;*@&nbsp;y')&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;(transpose&nbsp;x)&nbsp;$@&nbsp;z&nbsp;/$&nbsp;(float_of_int&nbsp;(row_num&nbsp;x))

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> squared hinge loss <span class="keyword">function</span> </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;hinge2_loss&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;hinge_loss&nbsp;y&nbsp;y'&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="constructor">MX</span>.(z&nbsp;*@&nbsp;z)

<span class="keyword">let</span>&nbsp;hinge2_grad&nbsp;x&nbsp;y&nbsp;y'&nbsp;=&nbsp;<span class="constructor">None</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> softmax loss <span class="keyword">function</span> </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;softmax_loss&nbsp;y&nbsp;y'&nbsp;=&nbsp;<span class="constructor">None</span>

<span class="keyword">let</span>&nbsp;softmax_grad&nbsp;x&nbsp;y&nbsp;y'&nbsp;=&nbsp;<span class="constructor">None</span>

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> logistic loss <span class="keyword">function</span> </code>  *)</span></td></tr></table><code class="code">

<span class="keyword">let</span>&nbsp;log_loss&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;z&nbsp;=&nbsp;<span class="constructor">MX</span>.map&nbsp;(<span class="keyword">fun</span>&nbsp;x&nbsp;<span class="keywordsign">-&gt;</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">if</span>&nbsp;x&nbsp;&gt;&nbsp;18.&nbsp;<span class="keyword">then</span>&nbsp;exp&nbsp;(-1.&nbsp;*.&nbsp;x)
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;<span class="keyword">if</span>&nbsp;x&nbsp;&lt;&nbsp;(-18.)&nbsp;<span class="keyword">then</span>&nbsp;(-1.&nbsp;*.&nbsp;x)
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;log&nbsp;(1.&nbsp;+.&nbsp;exp(-1.&nbsp;*.&nbsp;x))
&nbsp;&nbsp;)&nbsp;<span class="constructor">MX</span>.(y&nbsp;*@&nbsp;y')&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="constructor">MX</span>.average_rows&nbsp;z

<span class="keyword">let</span>&nbsp;log_grad&nbsp;x&nbsp;y&nbsp;y'&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;<span class="keyword">open</span>&nbsp;<span class="constructor">MX</span>&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;y'&nbsp;=&nbsp;sigmoid&nbsp;y'&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;(transpose&nbsp;x)&nbsp;$@&nbsp;(y'&nbsp;-@&nbsp;y)&nbsp;/$&nbsp;(float_of_int&nbsp;(row_num&nbsp;x))


<span class="comment">(*&nbsp;Stochastic&nbsp;Gradient&nbsp;Descent&nbsp;related&nbsp;functions&nbsp;*)</span>

<span class="keyword">let</span>&nbsp;constant_rate&nbsp;a&nbsp;s&nbsp;c&nbsp;=&nbsp;0.1

<span class="keyword">let</span>&nbsp;optimal_rate&nbsp;a&nbsp;s&nbsp;c&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;c&nbsp;=&nbsp;float_of_int&nbsp;c&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;s&nbsp;/.&nbsp;((1.&nbsp;+.&nbsp;(a&nbsp;*.&nbsp;s&nbsp;*.&nbsp;c))&nbsp;**&nbsp;0.75)

<span class="keyword">let</span>&nbsp;decr_rate&nbsp;a&nbsp;s&nbsp;c&nbsp;=&nbsp;min&nbsp;0.5&nbsp;(1.&nbsp;/.&nbsp;(a&nbsp;*.&nbsp;float_of_int&nbsp;c))

<span class="keyword">let</span>&nbsp;when_stable&nbsp;v&nbsp;c&nbsp;=&nbsp;v&nbsp;&lt;&nbsp;0.00001

<span class="keyword">let</span>&nbsp;when_enough&nbsp;v&nbsp;c&nbsp;=&nbsp;(v&nbsp;&lt;&nbsp;0.00001&nbsp;<span class="keywordsign">&amp;&amp;</span>&nbsp;c&nbsp;&gt;&nbsp;1000)&nbsp;<span class="keywordsign">||</span>&nbsp;(c&nbsp;&gt;&nbsp;5000)


</code><table><tr><td></td><td><span class="comment">(** <code class="code">
  <span class="constructor">Stochastic</span> <span class="constructor">Gradient</span> <span class="constructor">Descent</span> (<span class="constructor">SGD</span>) algorithm
  b : batch size
  s : step size
  t : stop criteria
  l : loss <span class="keyword">function</span>
  g : gradient <span class="keyword">function</span> <span class="keyword">of</span> the loss <span class="keyword">function</span>
  r : regularisation <span class="keyword">function</span>
  o : gradient fucntion <span class="keyword">of</span> the regularisation <span class="keyword">function</span>
  a : weight on the regularisation term, common setting is 0.0001
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept <span class="keyword">or</span> not, default value is <span class="keyword">false</span>
  p : model parameters (k * m), each column is a classifier. <span class="constructor">So</span> we have m classifier <span class="keyword">of</span> k features.
  x : data matrix (n x k), each row is a data point. <span class="constructor">So</span> we have n datea points <span class="keyword">of</span> k features each.
  y : labeled data (n x m), n data points <span class="keyword">and</span> each is labeled <span class="keyword">with</span> m classifiers
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="comment">(*&nbsp;check&nbsp;whether&nbsp;the&nbsp;intercept&nbsp;is&nbsp;needed&nbsp;or&nbsp;not&nbsp;*)</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;p&nbsp;=&nbsp;<span class="keyword">if</span>&nbsp;i&nbsp;=&nbsp;<span class="keyword">false</span>&nbsp;<span class="keyword">then</span>&nbsp;ref&nbsp;p
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;ref&nbsp;<span class="constructor">MX</span>.(p&nbsp;@=&nbsp;uniform&nbsp;1&nbsp;(col_num&nbsp;p))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;x&nbsp;=&nbsp;<span class="keyword">if</span>&nbsp;i&nbsp;=&nbsp;<span class="keyword">false</span>&nbsp;<span class="keyword">then</span>&nbsp;x
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">else</span>&nbsp;<span class="constructor">MX</span>.(x&nbsp;@||&nbsp;ones&nbsp;(row_num&nbsp;x)&nbsp;1)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;st&nbsp;=&nbsp;ref&nbsp;0.1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;cost&nbsp;=&nbsp;ref&nbsp;(<span class="constructor">Array</span>.make&nbsp;5000&nbsp;0.)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;obj0&nbsp;=&nbsp;ref&nbsp;max_float&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;obj1&nbsp;=&nbsp;ref&nbsp;min_float&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;counter&nbsp;=&nbsp;ref&nbsp;0&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">while</span>&nbsp;not&nbsp;(t&nbsp;(abs_float&nbsp;(!obj1&nbsp;-.&nbsp;!obj0))&nbsp;!counter)&nbsp;<span class="keyword">do</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;obj0&nbsp;:=&nbsp;!obj1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="comment">(*&nbsp;draw&nbsp;random&nbsp;samples&nbsp;for&nbsp;data&nbsp;*)</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;xt,&nbsp;idx&nbsp;=&nbsp;<span class="constructor">MX</span>.draw_rows&nbsp;x&nbsp;b&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;yt&nbsp;=&nbsp;<span class="constructor">MX</span>.rows&nbsp;y&nbsp;idx&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="comment">(*&nbsp;predict&nbsp;then&nbsp;estimate&nbsp;the&nbsp;loss&nbsp;and&nbsp;gradient&nbsp;*)</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;yt'&nbsp;=&nbsp;<span class="constructor">MX</span>.(xt&nbsp;$@&nbsp;!p)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;lt&nbsp;=&nbsp;l&nbsp;yt&nbsp;yt'&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;dt&nbsp;=&nbsp;g&nbsp;xt&nbsp;yt&nbsp;yt'&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="comment">(*&nbsp;check&nbsp;if&nbsp;it&nbsp;is&nbsp;regularised&nbsp;*)</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;lt&nbsp;=&nbsp;<span class="keyword">if</span>&nbsp;a&nbsp;=&nbsp;0.&nbsp;<span class="keyword">then</span>&nbsp;lt&nbsp;<span class="keyword">else</span>&nbsp;<span class="constructor">MX</span>.(lt&nbsp;+@&nbsp;(a&nbsp;$*&nbsp;(r&nbsp;!p)))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;dt&nbsp;=&nbsp;<span class="keyword">if</span>&nbsp;a&nbsp;=&nbsp;0.&nbsp;<span class="keyword">then</span>&nbsp;dt&nbsp;<span class="keyword">else</span>&nbsp;<span class="constructor">MX</span>.(dt&nbsp;+@&nbsp;(a&nbsp;$*&nbsp;(o&nbsp;!p)))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="comment">(*&nbsp;update&nbsp;the&nbsp;gradient&nbsp;with&nbsp;step&nbsp;size&nbsp;*)</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;st&nbsp;:=&nbsp;s&nbsp;a&nbsp;!st&nbsp;!counter&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;p&nbsp;:=&nbsp;<span class="constructor">MX</span>.(!p&nbsp;-@&nbsp;(dt&nbsp;*$&nbsp;!st))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;obj1&nbsp;:=&nbsp;<span class="constructor">MX</span>.sum&nbsp;lt&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;<span class="keyword">if</span>&nbsp;!counter&nbsp;&lt;&nbsp;(<span class="constructor">Array</span>.length&nbsp;!cost)&nbsp;<span class="keyword">then</span>&nbsp;!cost.(!counter)&nbsp;&lt;-&nbsp;!obj1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;_&nbsp;=&nbsp;counter&nbsp;:=&nbsp;!counter&nbsp;+&nbsp;1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="constructor">Log</span>.info&nbsp;<span class="string">"iteration&nbsp;#%i:&nbsp;%.4f"</span>&nbsp;!counter&nbsp;!obj1;
&nbsp;&nbsp;&nbsp;&nbsp;flush&nbsp;stdout
&nbsp;&nbsp;<span class="keyword">done</span>;&nbsp;!p

</code><table><tr><td></td><td><span class="comment">(** <code class="code">
  wrapper <span class="keyword">of</span> the _sgd_basic fucntion
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;sgd&nbsp;?(b=1)&nbsp;?(s=optimal_rate)&nbsp;?(t=when_stable)&nbsp;?(l=square_loss)&nbsp;?(g=square_grad)&nbsp;?(r=noreg)&nbsp;?(o=noreg_grad)&nbsp;?(a=0.)&nbsp;?(i=<span class="keyword">false</span>)&nbsp;p&nbsp;x&nbsp;y&nbsp;=&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y

<span class="keyword">let</span>&nbsp;gradient_descent&nbsp;=&nbsp;<span class="constructor">None</span>

<span class="comment">(*&nbsp;TODO:&nbsp;wrap&nbsp;parameters&nbsp;into&nbsp;a&nbsp;record&nbsp;type&nbsp;*)</span>


</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Support</span> <span class="constructor">Vector</span> <span class="constructor">Machine</span> regression
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept bias <span class="keyword">in</span> parameters
  note that the values <span class="keyword">in</span> y are either +1 <span class="keyword">or</span> -1.
 </code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;svm_regression&nbsp;?(i=<span class="keyword">false</span>)&nbsp;p&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;b&nbsp;=&nbsp;min&nbsp;50&nbsp;(<span class="constructor">MX</span>.(row_num&nbsp;x)&nbsp;/&nbsp;2)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;s&nbsp;=&nbsp;decr_rate&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;t&nbsp;=&nbsp;when_enough&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;hinge_loss&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;hinge_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;r&nbsp;=&nbsp;l2&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;o&nbsp;=&nbsp;l2_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;a&nbsp;=&nbsp;1.&nbsp;/.&nbsp;(float_of_int&nbsp;100)&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Ordinary</span> <span class="constructor">Least</span> <span class="constructor">Square</span> regression
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept bias <span class="keyword">in</span> parameters
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;ols_regression&nbsp;?(i=<span class="keyword">true</span>)&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;b&nbsp;=&nbsp;1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;s&nbsp;=&nbsp;optimal_rate&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;t&nbsp;=&nbsp;when_stable&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;square_loss&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;square_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;r&nbsp;=&nbsp;noreg&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;o&nbsp;=&nbsp;noreg_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;a&nbsp;=&nbsp;0.&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(uniform&nbsp;(col_num&nbsp;x)&nbsp;(col_num&nbsp;y))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Ridge</span> regression
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept bias <span class="keyword">in</span> parameters
  a : weight on the regularisation term
  <span class="constructor">TODO</span>: how <span class="keyword">to</span> choose a automatically
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;ridge_regression&nbsp;?(i=<span class="keyword">true</span>)&nbsp;?(a=0.001)&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;b&nbsp;=&nbsp;1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;s&nbsp;=&nbsp;optimal_rate&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;t&nbsp;=&nbsp;when_stable&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;square_loss&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;square_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;r&nbsp;=&nbsp;l2&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;o&nbsp;=&nbsp;l2_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(uniform&nbsp;(col_num&nbsp;x)&nbsp;(col_num&nbsp;y))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Lasso</span> regression
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept bias <span class="keyword">in</span> parameters
  a : weight on the regularisation term
  <span class="constructor">TODO</span>: how <span class="keyword">to</span> choose a automatically
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;lasso_regression&nbsp;?(i=<span class="keyword">true</span>)&nbsp;?(a=0.001)&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;b&nbsp;=&nbsp;1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;s&nbsp;=&nbsp;optimal_rate&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;t&nbsp;=&nbsp;when_stable&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;square_loss&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;square_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;r&nbsp;=&nbsp;l1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;o&nbsp;=&nbsp;l1_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(uniform&nbsp;(col_num&nbsp;x)&nbsp;(col_num&nbsp;y))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y

</code><table><tr><td></td><td><span class="comment">(** <code class="code"> <span class="constructor">Logistic</span> regression
  i : whether <span class="keyword">to</span> <span class="keyword">include</span> intercept bias <span class="keyword">in</span> parameters
  a : weight on the regularisation term
  note that the values <span class="keyword">in</span> y are either +1 <span class="keyword">or</span> 0.
</code>  *)</span></td></tr></table><code class="code">
<span class="keyword">let</span>&nbsp;logistic_regression&nbsp;?(i=<span class="keyword">true</span>)&nbsp;x&nbsp;y&nbsp;=
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;b&nbsp;=&nbsp;1&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;s&nbsp;=&nbsp;optimal_rate&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;t&nbsp;=&nbsp;when_stable&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;l&nbsp;=&nbsp;log_loss&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;g&nbsp;=&nbsp;log_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;r&nbsp;=&nbsp;noreg&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;o&nbsp;=&nbsp;noreg_grad&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;a&nbsp;=&nbsp;0.&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;<span class="keyword">let</span>&nbsp;p&nbsp;=&nbsp;<span class="constructor">MX</span>.(uniform&nbsp;(col_num&nbsp;x)&nbsp;(col_num&nbsp;y))&nbsp;<span class="keyword">in</span>
&nbsp;&nbsp;_sgd_basic&nbsp;b&nbsp;s&nbsp;t&nbsp;l&nbsp;g&nbsp;r&nbsp;o&nbsp;a&nbsp;i&nbsp;p&nbsp;x&nbsp;y





<span class="comment">(*&nbsp;TODO:&nbsp;'lbfgs',&nbsp;'newton-cg',&nbsp;'liblinear',&nbsp;'sag'&nbsp;solvers&nbsp;*)</span>
<span class="comment">(*&nbsp;TODO:&nbsp;Simulated&nbsp;Annealing&nbsp;*)</span>

<span class="comment">(*&nbsp;ends&nbsp;here&nbsp;*)</span>
</code></body></html>